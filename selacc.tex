\documentclass[modern]{aastex62}

\usepackage{amsmath}
\usepackage{amssymb}

\newcommand{\dd}{\mathrm{d}}
\newcommand{\diff}[2]{\frac{\dd #1}{\dd #2}}
\newcommand{\fixme}[1]{\textcolor{red}{#1}}
\newcommand{\Ndraw}{N_\mathrm{draw}}
\newcommand{\Ndet}{N_\mathrm{det}}
\newcommand{\Neff}{N_\mathrm{eff}}
\newcommand{\Nobs}{N_\mathrm{obs}}
\newcommand{\pdraw}{p_\mathrm{draw}}

\begin{document}

\title{Accuracy Requirements for Empirically-Measured Selection Functions}

\author[0000-0003-1540-8562]{Will M. Farr}
\email{will.farr@stonybrook.edu}
\email{wfarr-vscholar@flatironinstitute.org}
\affiliation{Department of Physics and Astronomy, Stony Brook University, Stony Brook NY 11794, United States}
\affiliation{Center for Computational Astronomy, Flatiron Institute, New York NY 10010, United States}

\maketitle

When conducting a population analysis on a catalog of objects the effect of the
selection function---the choice of which observations produce objects that
appear in the catalog and which do not---must be determined to sufficient
accuracy to avoid so-called ``Malmquist bias''
\citep{Malmquist1922,Loredo2004,Mandel2018}. Suppose we have a catalog
consisting of measurements (data) $d_i$, $i = 1, \ldots, \Nobs$, that constrain
the parameters $\theta_i$ of a set of $\Nobs$ objects.  We wish to use this
catalog to infer the population distribution function
%
\begin{equation}
  \diff{N}{\theta}\left( \lambda \right),
\end{equation}
%
which can depend on some population-level parameters $\lambda$.  The joint
posterior for the object-level parameters $\theta_i$ and population-level
parameters takes the form \citep{Loredo2004,Mandel2018}
%
\begin{equation}
  \label{eq:posterior}
\pi \propto \prod_{i=1}^{\Nobs} \left[ p\left( d_i \mid \theta_i \right) \diff{N}{\theta_i}\left( \lambda \right) \right] \exp\left[ - \Lambda\left( \lambda \right) \right] p\left( \lambda \right).
\end{equation}
%
Here $p\left( d \mid \theta\right)$ is the likelihood function that describes
the measurement process for the catalog, $p\left( \lambda \right)$ is a prior on
the population-level parameters, and $\Lambda$ is the expected number of
detections given the measurement process and selection effects, here assumed to
depend only on the observed data for each object:
%
\begin{equation}
  \label{eq:selection-integral-unnorm}
  \Lambda\left( \lambda \right) \equiv \int_{\left\{ d \mid f(d) > 0 \right\}} \dd d \, \dd \theta \, \diff{N}{\theta}\left( \lambda \right) p\left( d \mid \theta \right).
\end{equation}
%
Here $f$ is the selection function, such that an observation will be included in
the catalog if and only if it generates data such that $f(d) > 0$.  It will be
convenient to factor an overall normalization out of the population distribution
so that
%
\begin{equation}
  \diff{N}{\theta}\left( \lambda \right) = R \xi\left( \theta \mid \tilde{\lambda} \right),
\end{equation}
%
with the amplitude of $\xi$ fixed in some way (perhaps by demanding that $\xi$
integrate to one over all $\theta$, or fixing its value at a fiducial set of
parameters); $\tilde{\lambda}$ is the set of parameters that remain once the
amplitude of the population distribution is fixed.  In this re-parameterization,
$\Lambda = R x$, where $x$ is given by
%
\begin{equation}
  \label{eq:selection-integral}
  x\left( \tilde{\lambda} \right) \equiv \int_{\left\{ d \mid f(d) > 0 \right\}} \dd d \, \dd \theta \, \xi\left( \theta \mid \tilde{\lambda} \right) p\left( d \mid \theta \right).
\end{equation}
%
If $\xi$ integrates to one over all $\theta$, then $x$ is the \emph{fraction} of
sources from a population described by $\tilde{\lambda}$ that are detectable.

In simple cases the integral in Eq.\ \eqref{eq:selection-integral} can be
evaluated analytically.  In other cases it is not possible to evaluate the
integral analytically, but $f$ is a simple enough function to permit evaluation
of the integral stochastically in the process of sampling from $\pi$
\citep{Mandel2018}.  But for most realistic applications it is not even possible
to analytically evaluate $f$ \citep[see
e.g.][]{Burke2015,Christainsen2015,GW150914-Rate,GW150914-Rate-Supplement,Burke2017}.
Instead, the detection efficiency must be \emph{simulated} by drawing synthetic
objects from a fiducial distribution, $\pdraw\left( \theta \right)$, drawing
corresponding data from the likelihood function $p\left( d \mid \theta \right)$,
and ``injecting'' these data into the pipeline used to produce the catalog,
recording which observations are detected.  This Monte-Carlo procedure
introduces uncertainty in the estimation of the selection integral; we must have
enough draws that this uncertainty does not alter the shape of the posterior
$\pi$ very much.

Given a set of detected objects with parameters $\theta_j$, $j = 1, \ldots,
\Ndet$, generated from a total number of draws $\Ndraw$ the integral in Eq.\
\eqref{eq:selection-integral} can be estimated via
%
\begin{equation}
  x \simeq \frac{1}{\Ndraw} \sum_{j=1}^{\Ndet} \frac{\xi\left( \theta_j \mid \tilde{\lambda} \right)}{\pdraw\left( \theta_j \right)}.
\end{equation}
%
In fact, under repeated samplings and assuming a large number of effective draws
(see below) $x$ will follow an approximately log-normal
distribution\footnote{The law of large numbers would suggest that $x$ follows a
Gaussian distribution.  Here we use a log-normal because $x$ cannot follow a
Gaussian in detail ($x > 0$ with probability 1), explicitly incorporating $x >
0$ eliminates an artificial divergence as $x \to 0$, and in the asymptotic
regime (which is what we are concerned with in this note) the behavior of the
two distributions is identical.}
%
\begin{equation}
    \log x \sim N\left( \mu, \sigma \right),
\end{equation}
%
with
%
\begin{equation}
  \exp\left[\mu\right] \simeq \frac{1}{\Ndraw} \sum_{j=1}^{\Ndet} \frac{\xi\left( \theta_j \mid \tilde{\lambda} \right)}{\pdraw\left( \theta_j \right)} ,
\end{equation}
%
and
%
\begin{equation}
    \sigma^2 \equiv \frac{1}{\Neff} \simeq \frac{\exp[-2\mu]}{\Ndraw^2} \sum_{i=1}^{\Ndet} \left[ \frac{\xi\left( \theta_j \mid \tilde{\lambda} \right)}{\pdraw\left( \theta_j \right)} \right]^2 - \frac{1}{\Ndraw},
\end{equation}
%
where we have introduced the parameter $\Neff$ that gives the \emph{effective}
number of independent samples (draws) that contribute to the Monte-Carlo
estimate of $x$.

Given a particular sampling of the selection function, we should marginalize
over the uncertainty in $x$ in our posterior.  It will be convenient to choose a
flat-in-log prior on $R$, so that $p(R) \propto R^{-1}$
\citep{Fishbach2018,Mandel2018}.  Then Eq.\ \eqref{eq:posterior} becomes
%
\begin{equation}
  \label{eq:posterior-integrated}
    \pi \propto \prod_{i = 1}^{\Nobs} \left[ p\left( d_i \mid \theta_i \right) \xi\left( \theta_i \mid \tilde{\lambda} \right) \right] \int \dd x \, R^{\Nobs - 1} \exp\left[ -R x \right] \frac{N\left( \log x \mid \mu, \sigma\right)}{x}.
\end{equation}
%
The posterior can be simultaneously marginalized over $x$ and $R$, to obtain a
marginal posterior, $\bar{\pi}$, for parameters $\theta_i$ and $\tilde{\lambda}$
that is
%
\begin{equation}
  \log \bar{\pi} = \sum_{i = 1}^{\Nobs} \log \left[ p\left( d_i \mid \theta_i \right) \xi\left( \theta_i \mid \tilde{\lambda} \right) \right] - \Nobs \mu + \frac{\Nobs^2}{2 \Neff} + \mathrm{const}.
\end{equation}
%
The $-\Nobs\mu$ term corresponds to the normalization of $\Nobs$ likelihood
terms over \emph{selected} systems \citep{Mandel2018} assuming the selection
integral is known perfectly, while the $\Nobs^2/2\Neff$ term comes from the
Monte-Carlo uncertainty in the estimate of the integral.  At first glance, this
latter term might seem problematic unless $\Neff \ll \Nobs^2$ which, since
$\Neff = \mathcal{O}\left( \Ndraw \right)$, would imply that accurate parameter
estimates for large catalogs would require a quadratically-increasing number of
draws for the Monte-Carlo estimate of the selection integral\footnote{A simple
argument suggests that a quadratic scaling must be incorrect: for a quadratic
scaling it would be more computationally efficient to break the observed catalog
in half and fit the first half (using $1/4$ as many samples to evaluate the
selection integral), using the results of the fit as the prior for a fit of the
second half (with, again, $1/4$ as many samples).  But, statistically, this
procedure is equivalent to fitting the full catalog: if $d_1$ and $d_2$ are the
halves of the full data set, we have $p\left( d_1, d_2 \mid
\tilde{\lambda}\right) = p\left( d_1 \mid d_2, \tilde{\lambda}\right) p\left(
d_2 \mid \tilde{\lambda} \right)$.  If the scaling were quadratic, splitting
would be asymptotically more efficient in the number of draws from the selection
function while returning statistically identical estimates.}.  However, only
\emph{differences} in $\log \bar{\pi}$ that depend on $\tilde{\lambda}$ matter
for inferences on parameters, and these scale as
%
\begin{equation}
  \label{eq:pi-accuracy}
    \Delta \log \bar{\pi} \simeq \ldots + \left( \Nobs \frac{\partial \mu}{\partial \tilde{\lambda}} + \frac{\Nobs^2}{2\Neff} \frac{\partial \log \Neff}{\partial \tilde{\lambda}} \right) \Delta \tilde{\lambda}.
\end{equation}
%
We must draw enough samples in the Monte-Carlo selection estimate that the
second term in parenthesis is sub-dominant.  Since both partial derivatives are
independent of $\Neff$, we have
%
\begin{equation}
    \Neff \gtrsim A \frac{\Nobs}{2},
\end{equation}
%
where the coefficient $A$ depends on the ratio of derivative terms above but is
typically $\mathcal{O}(1)$.  The contribution of the two terms in Eq.\
\eqref{eq:pi-accuracy} can be straightforwardly tracked during, e.g., a MCMC
stochastic sampling of the posterior by comparing the posterior standard
deviation of $\Nobs \mu$ to that of $\Nobs^2/2\Neff$; if the latter is not
sub-dominant, then more samples are necessary to estimate the selection integral
for reliable parameter estimates.

If we wish to estimate the normalization $R$ in addition to the shape of the
population distribution, then we need to account for the shape of the integrand
in Eq.\ \eqref{eq:posterior-integrated}.  For sufficiently large $\Neff$ and
$\Nobs$, the integrand can be approximated as a Gaussian with a peak at $x =
\exp\left[ \mu - \Nobs/\Neff \right]$ and $R = \left( \Nobs - 1 \right)/x$ and a
covariance matrix given, to leading order, by
%
\begin{equation}
  \Sigma = \begin{pmatrix} \Sigma_{RR} & \Sigma_{Rx} \\ \Sigma_{xR} & \Sigma_{xx} \end{pmatrix} = \begin{pmatrix} \exp\left[ - 2 \mu \right] \frac{\left(\Nobs - 1 \right)\Neff + 1 - 4 \Nobs + 3 \Nobs^2}{\Neff} & \frac{1 - \Nobs}{\Neff} \\ \frac{1-\Nobs}{\Neff} & \frac{\exp\left[ 2 \mu \right]}{\Neff} \end{pmatrix}.
\end{equation}
%
We see that the fractional bias in $x$ (and therefore also $R$) is
$\mathcal{O}\left( \Nobs / \Neff \right)$.  The uncertainty on $R$ after
marginalizing over $x$ will be $\sigma_R = \sqrt{\Sigma_{RR}}$.  For reliable
estimation of $R$, this uncertainty should not be significantly larger than the
uncertainty assuming the selection integral is known perfectly, which is
$\sigma_R / R \simeq 1/\sqrt{\Nobs}$.  Therefore, for accurate estimation of the
normalization of the population distribution, we require $3\Nobs^2/\Neff \ll
\Nobs - 1$ or $\Neff \gg \Nobs$.  As before, it is straightforward to track the
fractional bias and increase in uncertainty in $R$ during stochastic sampling to
ensure the desired accuracy.

\bibliography{selacc}

\end{document}
